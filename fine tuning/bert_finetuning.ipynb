{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT FINETUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"https://raw.githubusercontent.com/laxmimerit/All-CSV-ML-Data-Files-Download/master/twitter_multi_class_sentiment.csv\"\n",
    "df: pd.DataFrame = pd.read_csv(filepath_or_buffer=DATA_PATH)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16000 entries, 0 to 15999\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   text        16000 non-null  object\n",
      " 1   label       16000 non-null  int64 \n",
      " 2   label_name  16000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 375.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>i just had a very brief time in the beanbag an...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>i am now turning and i feel pathetic that i am...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>i feel strong and good overall</td>\n",
       "      <td>1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>i feel like this was such a rude comment and i...</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>i know a lot but i feel so stupid because i ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label label_name\n",
       "0                                i didnt feel humiliated      0    sadness\n",
       "1      i can go from feeling so hopeless to so damned...      0    sadness\n",
       "2       im grabbing a minute to post i feel greedy wrong      3      anger\n",
       "3      i am ever feeling nostalgic about the fireplac...      2       love\n",
       "4                                   i am feeling grouchy      3      anger\n",
       "...                                                  ...    ...        ...\n",
       "15995  i just had a very brief time in the beanbag an...      0    sadness\n",
       "15996  i am now turning and i feel pathetic that i am...      0    sadness\n",
       "15997                     i feel strong and good overall      1        joy\n",
       "15998  i feel like this was such a rude comment and i...      3      anger\n",
       "15999  i know a lot but i feel so stupid because i ca...      0    sadness\n",
       "\n",
       "[16000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_name\n",
       "joy         33.51250\n",
       "sadness     29.16250\n",
       "anger       13.49375\n",
       "fear        12.10625\n",
       "love         8.15000\n",
       "surprise     3.57500\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label_name\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Frequency of Classes')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas import Series\n",
    "\n",
    "label_counts = df[\"label_name\"].value_counts(ascending=True)\n",
    "label_counts.plot.barh()\n",
    "plt.title(label=\"Frequency of Classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "      <th>Words per Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>i just had a very brief time in the beanbag an...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>i am now turning and i feel pathetic that i am...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>i feel strong and good overall</td>\n",
       "      <td>1</td>\n",
       "      <td>joy</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>i feel like this was such a rude comment and i...</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>i know a lot but i feel so stupid because i ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label label_name  \\\n",
       "0                                i didnt feel humiliated      0    sadness   \n",
       "1      i can go from feeling so hopeless to so damned...      0    sadness   \n",
       "2       im grabbing a minute to post i feel greedy wrong      3      anger   \n",
       "3      i am ever feeling nostalgic about the fireplac...      2       love   \n",
       "4                                   i am feeling grouchy      3      anger   \n",
       "...                                                  ...    ...        ...   \n",
       "15995  i just had a very brief time in the beanbag an...      0    sadness   \n",
       "15996  i am now turning and i feel pathetic that i am...      0    sadness   \n",
       "15997                     i feel strong and good overall      1        joy   \n",
       "15998  i feel like this was such a rude comment and i...      3      anger   \n",
       "15999  i know a lot but i feel so stupid because i ca...      0    sadness   \n",
       "\n",
       "       Words per Tweet  \n",
       "0                    4  \n",
       "1                   21  \n",
       "2                   10  \n",
       "3                   18  \n",
       "4                    4  \n",
       "...                ...  \n",
       "15995               24  \n",
       "15996               20  \n",
       "15997                6  \n",
       "15998               14  \n",
       "15999               15  \n",
       "\n",
       "[16000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Words per Tweet\"] = df[\"text\"].str.split().apply(func=len)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Words per Tweet'}, xlabel='label_name'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.boxplot(column=\"Words per Tweet\", by=\"label_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast = (\n",
    "    AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_checkpoint)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "{'input_ids': [101, 4931, 999, 2129, 2024, 2017, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "\n",
    "text = \"Hey! How are you?\"\n",
    "encoded_input: BatchEncoding = tokenizer(text)\n",
    "print(len(encoded_input))\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522, 30522, 512)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab), tokenizer.vocab_size, tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader and Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11200, 4), (3200, 4), (1600, 4))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.3, stratify=df[\"label_name\"])\n",
    "test, val = train_test_split(test, test_size=1 / 3, stratify=test[\"label_name\"])\n",
    "\n",
    "train.shape, test.shape, val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_name', 'Words per Tweet'],\n",
       "        num_rows: 11200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'label_name', 'Words per Tweet'],\n",
       "        num_rows: 3200\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'label', 'label_name', 'Words per Tweet'],\n",
       "        num_rows: 1600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_pandas(df=train, preserve_index=False),\n",
    "        \"test\": Dataset.from_pandas(df=test, preserve_index=False),\n",
    "        \"val\": Dataset.from_pandas(df=val, preserve_index=False),\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "\n",
    "def tokenize(batch) -> BatchEncoding:\n",
    "    return tokenizer(batch[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['i also have the feeling i need a very relaxed practice today', 'i wanted to make sure i didnt feel rushed getting to century college on friday afternoon'], 'label': [1, 3], 'label_name': ['joy', 'anger'], 'Words per Tweet': [12, 16]}\n",
      "{'input_ids': [[101, 1045, 2036, 2031, 1996, 3110, 1045, 2342, 1037, 2200, 8363, 3218, 2651, 102], [101, 1045, 2359, 2000, 2191, 2469, 1045, 2134, 2102, 2514, 6760, 2893, 2000, 2301, 2267, 2006, 5958, 5027, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0:2])\n",
    "print(tokenize(batch=dataset[\"train\"][0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046f25df36a643e68bda5e8ca8645aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c557281431dc49059348008b8a326728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5a049fc37749b78d2e439bdb0c4fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "\n",
    "\n",
    "encoded_data: DatasetDict = dataset.map(\n",
    "    function=tokenize, batched=True, batch_size=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_name', 'Words per Tweet', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 11200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'label_name', 'Words per Tweet', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3200\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'label', 'label_name', 'Words per Tweet', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'i also have the feeling i need a very relaxed practice today', 'label': 1, 'label_name': 'joy', 'Words per Tweet': 12}\n"
     ]
    }
   ],
   "source": [
    "for x in dataset[\"train\"]:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joy': 1, 'anger': 3, 'sadness': 0, 'fear': 4, 'love': 2, 'surprise': 5}\n",
      "{1: 'joy', 3: 'anger', 0: 'sadness', 4: 'fear', 2: 'love', 5: 'surprise'}\n"
     ]
    }
   ],
   "source": [
    "# mapping with label2id, id2label\n",
    "\n",
    "label2id = {x[\"label_name\"]: x[\"label\"] for x in dataset[\"train\"]}\n",
    "id2label = {x[\"label\"]: x[\"label_name\"] for x in dataset[\"train\"]}\n",
    "\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-uncased'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47646c59d0ac47aab0574bdf11ae3556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(pretrained_model_name_or_path=model_checkpoint)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BertForMaskedLM']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.46.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "num_labels: int = len(label2id)\n",
    "print(num_labels)\n",
    "\n",
    "device = torch.device(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_checkpoint,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_checkpoint, config=config\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"sadness\",\n",
       "    \"1\": \"joy\",\n",
       "    \"2\": \"love\",\n",
       "    \"3\": \"anger\",\n",
       "    \"4\": \"fear\",\n",
       "    \"5\": \"surprise\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"anger\": 3,\n",
       "    \"fear\": 4,\n",
       "    \"joy\": 1,\n",
       "    \"love\": 2,\n",
       "    \"sadness\": 0,\n",
       "    \"surprise\": 5\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.46.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "TRAINING_DIR = \"bert_base_train\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    output_dir=TRAINING_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy: evaluate.EvaluationModule = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics_evaluate(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(a=predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use skleaarn if not want to use evaluate\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def compute_metrics_sklearn(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_name', 'Words per Tweet'],\n",
       "        num_rows: 11200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'label_name', 'Words per Tweet'],\n",
       "        num_rows: 3200\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'label', 'label_name', 'Words per Tweet'],\n",
       "        num_rows: 1600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/3zwp4_s56jb51k20dlwb3v8h0000gn/T/ipykernel_12797/2543496065.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_data[\"train\"],\n",
    "    eval_dataset=encoded_data[\"val\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_sklearn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae685f7e214c4901969bfa37ce669b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/525 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/transformers/trainer.py:2122\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2120\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2123\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/transformers/trainer.py:2474\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2473\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2474\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2477\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2479\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2480\u001b[0m ):\n\u001b[1;32m   2481\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2482\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/transformers/trainer.py:3572\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3571\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3572\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3578\u001b[0m ):\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/transformers/trainer.py:3625\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3623\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3624\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3625\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3626\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1668\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1662\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1663\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1668\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1680\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1682\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/transformers/pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:640\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:553\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    552\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 553\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/llm-engineering-RAtdaHMm-py3.12/lib/python3.12/site-packages/torch/nn/functional.py:1268\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import PredictionOutput\n",
    "\n",
    "\n",
    "preds_output: PredictionOutput = trainer.predict(test_dataset=encoded_data[\"test\"])\n",
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(preds_output.predictions, axis=1)\n",
    "y_true = encoded_data[\"test\"][:][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[0;32m----> 3\u001b[0m classification_report(\u001b[43my_true\u001b[49m, y_pred)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(y_true\u001b[38;5;241m=\u001b[39m\u001b[43my_true\u001b[49m, y_pred\u001b[38;5;241m=\u001b[39my_pred)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      8\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(\n\u001b[1;32m      9\u001b[0m     data\u001b[38;5;241m=\u001b[39mcm,\n\u001b[1;32m     10\u001b[0m     annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReds\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(\n",
    "    data=cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    xticklabels=label2id.keys(),\n",
    "    yticklabels=label2id.keys(),\n",
    "    cbar=False,\n",
    "    cmap=\"Reds\",\n",
    ")\n",
    "plt.title(label=\"Confusion matrix\")\n",
    "plt.ylabel(ylabel=\"Actual label\")\n",
    "plt.xlabel(xlabel=\"Predicted label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2572, 3565, 3407, 2651, 1012, 1045, 2031, 2288, 2009, 2589,\n",
       "         1012, 2633,  999,  999,  999,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am super happy today. I have got it done. Finally!!!\"\n",
    "\n",
    "\n",
    "def get_prediction(text):\n",
    "    input_encoded = tokenizer(text, return_tensors=\"pt\").to(device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_encoded)\n",
    "\n",
    "    logits = outputs.logits\n",
    "\n",
    "    pred = torch.argmax(logits, axis=1).item()\n",
    "    return id2label[pred]\n",
    "\n",
    "\n",
    "get_prediction(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"bert-base-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"bert-base-uncased-sentiment\")\n",
    "print(classifier(text))\n",
    "print(classifier(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Model to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, HfApi, Repository\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "def push_to_hub(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    model_name: str,\n",
    "    username: str,\n",
    "    commit_message: str = \"Add fine-tuned model\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Push a fine-tuned model and tokenizer to Hugging Face Model Hub.\n",
    "\n",
    "    Args:\n",
    "    model (PreTrainedModel): The fine-tuned model to push.\n",
    "    tokenizer (PreTrainedTokenizer): The tokenizer associated with the model.\n",
    "    model_name (str): The name to give to the model on the Hub.\n",
    "    username (str): Your Hugging Face username.\n",
    "    commit_message (str, optional): The commit message for the push. Defaults to \"Add fine-tuned model\".\n",
    "\n",
    "    Returns:\n",
    "    str: The URL of the pushed model on the Hugging Face Model Hub.\n",
    "    \"\"\"\n",
    "    # Log in to Hugging Face\n",
    "    login()\n",
    "\n",
    "    # Set up repository information\n",
    "    repo_name = f\"{username}/{model_name}\"\n",
    "\n",
    "    # Create a new repository on Hugging Face\n",
    "    api = HfApi()\n",
    "    api.create_repo(repo_id=repo_name, exist_ok=True)\n",
    "\n",
    "    # Clone the repository locally\n",
    "    repo = Repository(local_dir=model_name, clone_from=repo_name)\n",
    "\n",
    "    # Save your model and tokenizer\n",
    "    model.save_pretrained(model_name)\n",
    "    tokenizer.save_pretrained(model_name)\n",
    "\n",
    "    # Push the files to your Hugging Face repository\n",
    "    repo.git_add()\n",
    "    repo.git_commit(commit_message)\n",
    "    repo.git_push()\n",
    "\n",
    "    return f\"Model pushed to https://huggingface.co/{repo_name}\"\n",
    "\n",
    "\n",
    "result = push_to_hub(model, tokenizer, \"my-awesome-model\", \"my-username\")\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering-RAtdaHMm-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
